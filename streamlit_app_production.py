#!/usr/bin/env python3
"""
Semantic Proximity Analyzer - Streamlit Production App
Full-featured semantic analysis with embeddings and modern visualizations
Deploy on: https://streamlit.io/cloud
"""

import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import tempfile
import textwrap
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import unicodedata
from translations import get_text


APP_I18N = {
    'en': {
        'lang_selector': 'Language',
        'how_it_works': 'How it works',
        'required_files_info': 'Required Files: Upload both your GSC export CSV and your strategic priority keywords file (Excel or CSV)',
        'upload_gsc': 'Upload GSC CSV',
        'upload_keywords': 'Upload Keywords File',
        'matching_sensitivity': 'Keyword Matching Sensitivity',
        'strict': 'Strict',
        'balanced': 'Balanced',
        'wide': 'Wide',
        'visualizations': 'Visualizations',
        'client_graphs': 'Client Insights Graphs',
        'technical_graphs': 'Technical Graphs',
        'graph_opportunity': 'Top SEO Opportunities',
        'graph_position': 'Position vs Semantic Score',
        'graph_bucket': 'Semantic Quality Mix',
        'opportunity_idx': 'Opportunity Index',
        'serp_position': 'SERP Position (lower is better)',
        'count': 'Count',
    },
    'fr': {
        'lang_selector': 'Langue',
        'how_it_works': 'Comment Ã§a marche',
        'required_files_info': 'Fichiers requis : importez votre export GSC CSV et votre fichier de mots-clÃ©s stratÃ©giques (Excel ou CSV)',
        'upload_gsc': 'Importer le CSV GSC',
        'upload_keywords': 'Importer le fichier de mots-clÃ©s',
        'matching_sensitivity': 'SensibilitÃ© du matching des mots-clÃ©s',
        'strict': 'Strict',
        'balanced': 'Ã‰quilibrÃ©',
        'wide': 'Large',
        'visualizations': 'Visualisations',
        'client_graphs': 'Graphes orientÃ©s client',
        'technical_graphs': 'Graphes techniques',
        'graph_opportunity': 'Top opportunitÃ©s SEO',
        'graph_position': 'Position vs Score sÃ©mantique',
        'graph_bucket': 'RÃ©partition qualitÃ© sÃ©mantique',
        'opportunity_idx': "Indice d'opportunitÃ©",
        'serp_position': 'Position SERP (plus bas = meilleur)',
        'count': 'Volume',
    },
    'ar': {
        'lang_selector': 'Ø§Ù„Ù„ØºØ©',
        'how_it_works': 'ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø§Ù„Ø£Ø¯Ø§Ø©',
        'required_files_info': 'Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: Ø§Ø±ÙØ¹ Ù…Ù„Ù GSC CSV ÙˆÙ…Ù„Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© (Excel Ø£Ùˆ CSV)',
        'upload_gsc': 'Ø±ÙØ¹ Ù…Ù„Ù GSC CSV',
        'upload_keywords': 'Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©',
        'matching_sensitivity': 'Ø­Ø³Ø§Ø³ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©',
        'strict': 'ØµØ§Ø±Ù…',
        'balanced': 'Ù…ØªÙˆØ§Ø²Ù†',
        'wide': 'ÙˆØ§Ø³Ø¹',
        'visualizations': 'Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©',
        'client_graphs': 'Ø±Ø³ÙˆÙ… Ù…ÙˆØ¬Ù‡Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„',
        'technical_graphs': 'Ø±Ø³ÙˆÙ… ØªÙ‚Ù†ÙŠØ©',
        'graph_opportunity': 'Ø£ÙØ¶Ù„ ÙØ±Øµ SEO',
        'graph_position': 'Ø§Ù„ØªØ±ØªÙŠØ¨ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©',
        'graph_bucket': 'ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©',
        'opportunity_idx': 'Ù…Ø¤Ø´Ø± Ø§Ù„ÙØ±ØµØ©',
        'serp_position': 'ØªØ±ØªÙŠØ¨ SERP (Ø§Ù„Ø£Ù‚Ù„ Ø£ÙØ¶Ù„)',
        'count': 'Ø§Ù„Ø¹Ø¯Ø¯',
    },
}


def t(language, key):
    if key in APP_I18N.get(language, {}):
        return APP_I18N[language][key]
    return get_text(language, key)

# ============= PAGE CONFIG =============
st.set_page_config(
    page_title="Semantic Proximity Analyzer",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ============= CUSTOM STYLING =============
st.markdown("""
<style>
    body { background-color: #ffffff; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; }
    h1 { font-weight: 700; color: #0f172a; font-size: 2.2rem; margin-bottom: 0.3rem; }
    h2 { font-weight: 600; color: #0f172a; font-size: 1.6rem; margin-top: 2rem; margin-bottom: 0.8rem; }
    h3 { font-weight: 600; color: #111827; font-size: 1.1rem; }
    .stButton>button { background-color: #1f2937; color: white; border: none; padding: 10px 20px; 
                       font-weight: 600; border-radius: 8px; transition: all 0.2s; }
    .stButton>button:hover { background-color: #111827; }
    .metric-card { background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%); padding: 20px; 
                   border-radius: 10px; border-left: 4px solid #0284c7; }
    .info-box { background: #f8f8f8; padding: 16px; border-radius: 8px; border-left: 4px solid #1f2937; }
</style>
""", unsafe_allow_html=True)

# ============= LOAD MODEL (CACHED) =============
@st.cache_resource
def load_embedding_model():
    try:
        return SentenceTransformer('all-MiniLM-L6-v2')
    except Exception as e:
        st.error(f"Failed to load embedding model: {e}")
        return None

# ============= UTILITY FUNCTIONS =============
def normalize_text(text):
    if pd.isna(text):
        return ""
    value = str(text).lower().strip()
    value = unicodedata.normalize('NFKD', value)
    value = ''.join(ch for ch in value if not unicodedata.combining(ch))
    value = re.sub(r"[^a-z0-9\s]", " ", value)
    value = re.sub(r"\s+", " ", value).strip()
    return value


def detect_column(columns, candidates):
    normalized = {col: normalize_text(col) for col in columns}
    for candidate in candidates:
        candidate_norm = normalize_text(candidate)
        for original, norm in normalized.items():
            if norm == candidate_norm or candidate_norm in norm:
                return original
    return None


def extract_keywords_from_dataframe(df):
    if df is None or df.empty:
        return []

    keyword_col = detect_column(
        df.columns,
        ['mot cle', 'mot-clÃ©', 'mot clÃ©', 'keyword', 'keywords', 'query']
    )

    if keyword_col is None:
        if len(df.columns) > 1:
            keyword_col = df.columns[1]
        else:
            keyword_col = df.columns[0]

    values = df[keyword_col].dropna().astype(str).tolist()
    cleaned = []
    for value in values:
        text = value.strip()
        if not text or len(text) < 2 or text.isdigit():
            continue
        normalized = normalize_text(text)
        if normalized in {'mot cle', 'keyword', 'keywords', 'nan'}:
            continue
        cleaned.append(text)
    return cleaned


def load_keywords_excel(file_obj):
    keywords = []
    try:
        filename = (getattr(file_obj, 'name', '') or '').lower()

        if filename.endswith('.csv'):
            file_obj.seek(0)
            df = pd.read_csv(file_obj, sep=None, engine='python')
            keywords.extend(extract_keywords_from_dataframe(df))

            if len(keywords) < 3:
                file_obj.seek(0)
                df_no_header = pd.read_csv(file_obj, sep=None, engine='python', header=None)
                keywords.extend(extract_keywords_from_dataframe(df_no_header))
        else:
            file_obj.seek(0)
            xls = pd.ExcelFile(file_obj)
            for sheet_name in xls.sheet_names:
                file_obj.seek(0)
                df = pd.read_excel(file_obj, sheet_name=sheet_name)
                keywords.extend(extract_keywords_from_dataframe(df))

                file_obj.seek(0)
                df_skip = pd.read_excel(file_obj, sheet_name=sheet_name, skiprows=3)
                keywords.extend(extract_keywords_from_dataframe(df_skip))

    except Exception as e:
        st.warning(f"Could not parse keywords file: {e}")

    seen = set()
    unique_keywords = []
    for item in keywords:
        key = normalize_text(item)
        if key and key not in seen:
            seen.add(key)
            unique_keywords.append(item.strip())
    return unique_keywords

def parse_french_number(val):
    """Convert French formatted numbers to float (e.g., '1 234,5' â†’ 1234.5)"""
    if pd.isna(val) or val == '':
        return 0.0
    val = str(val).strip()
    # Remove spaces (French thousands separator)
    val = val.replace(' ', '')
    val = val.replace('\u00a0', '')
    val = val.replace('\u202f', '')
    # Replace comma with dot (French decimal)
    val = val.replace(',', '.')
    val = val.replace('%', '')
    try:
        return float(val)
    except:
        return 0.0

def prepare_gsc_data(gsc_df):
    prepared = gsc_df.copy()

    query_col = detect_column(prepared.columns, ['query', 'queries', 'top queries'])
    page_col = detect_column(prepared.columns, ['page', 'landing page', 'url', 'top pages'])

    if not query_col or not page_col:
        return None, None, None

    for col in ['Clicks', 'Impressions', 'Position']:
        metric_col = detect_column(prepared.columns, [col])
        if metric_col:
            prepared[metric_col] = prepared[metric_col].apply(parse_french_number)

    prepared['_query_norm'] = prepared[query_col].apply(normalize_text)
    prepared = prepared[prepared['_query_norm'].str.len() > 0].copy()

    return prepared, query_col, page_col


def get_best_url_for_keyword(prepared_gsc_df, keyword, query_col, page_col, matching_mode='Balanced'):
    keyword_norm = normalize_text(keyword)
    if not keyword_norm:
        return None

    keyword_tokens = [token for token in keyword_norm.split() if len(token) > 1]
    if not keyword_tokens:
        return None

    mode = (matching_mode or 'Balanced').lower()
    if mode == 'strict':
        min_overlap = 0.75
    elif mode == 'wide':
        min_overlap = 0.30
    else:
        min_overlap = 0.45

    def keyword_match_score(query_norm):
        if not query_norm:
            return 0.0

        if keyword_norm == query_norm:
            return 1.0
        if keyword_norm in query_norm:
            return 0.95

        overlap = sum(1 for token in keyword_tokens if token in query_norm)
        overlap_ratio = overlap / len(keyword_tokens)
        if overlap_ratio >= min_overlap:
            return 0.5 + (overlap_ratio * 0.4)

        return 0.0

    scored = prepared_gsc_df.copy()
    scored['_match_score'] = scored['_query_norm'].apply(keyword_match_score)
    matched = scored[scored['_match_score'] > 0].copy()

    if matched.empty:
        return None

    clicks_col = detect_column(matched.columns, ['Clicks'])
    impressions_col = detect_column(matched.columns, ['Impressions'])
    position_col = detect_column(matched.columns, ['Position'])

    matched['_perf_score'] = (
        matched.get(clicks_col, 0) * 2
        + matched.get(impressions_col, 0) * 0.5
        - matched.get(position_col, 0) * 0.1
    )
    matched['_final_score'] = (matched['_match_score'] * 1000) + matched['_perf_score']
    best_row = matched.loc[matched['_final_score'].idxmax()]

    return {
        'url': str(best_row[page_col]),
        'query': str(best_row[query_col]),
        'clicks': int(best_row.get(clicks_col, 0)) if clicks_col else 0,
        'impressions': int(best_row.get(impressions_col, 0)) if impressions_col else 0,
        'position': float(best_row.get(position_col, 0)) if position_col else 0.0,
    }

def fetch_page_content(url):
    """Fetch page title, meta, and content"""
    try:
        request_url = url
        if not request_url.startswith('http'):
            request_url = 'https://' + request_url
        
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        response = requests.get(request_url, headers=headers, timeout=8, allow_redirects=True)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title = soup.title.string if soup.title else ""
        meta_desc = ""
        for meta in soup.find_all('meta'):
            if meta.get('name') == 'description':
                meta_desc = meta.get('content', '')
                break
        
        body = soup.find('body')
        text = ' '.join([p.get_text() for p in body.find_all('p')])[:500] if body else ""
        
        parsed = urlparse(request_url)
        url_text = (parsed.netloc + parsed.path)[:100]
        
        return {
            'title': title or "",
            'meta_description': meta_desc,
            'content': text,
            'url_text': url_text,
            'full_url': request_url,
        }, None
    except Exception as e:
        return None, f"{type(e).__name__}: {str(e)}"

def calculate_semantic_proximity(keyword, page_data, model):
    """Calculate semantic score using embeddings"""
    if not page_data or not model:
        return 0
    
    try:
        keyword_embedding = model.encode([keyword])[0]
        
        texts_to_compare = [
            page_data.get('title', ''),
            page_data.get('meta_description', ''),
            page_data.get('content', '')[:200],
            page_data.get('url_text', ''),
        ]
        
        page_embeddings = model.encode(texts_to_compare)
        
        similarities = []
        for emb in page_embeddings:
            sim = cosine_similarity([keyword_embedding], [emb])[0][0]
            similarities.append(max(0, sim))
        
        weights = [0.35, 0.25, 0.30, 0.10]
        score = sum(s * w for s, w in zip(similarities, weights)) * 100
        
        return round(score, 2)
    except:
        return 0

def generate_charts(result_df, labels=None):
    """Generate modern charts without text overlap"""
    charts = {}
    labels = labels or {}
    
    if result_df.empty:
        return charts
    
    plt.style.use('default')
    plt.rcParams.update({
        'font.size': 10,
        'axes.titlesize': 13,
        'axes.titleweight': 'bold',
        'axes.edgecolor': '#d1d5db',
        'grid.alpha': 0.3,
    })
    
    # 1) Quality Snapshot
    excellent = (result_df['Proximity_Score'] >= 80).sum()
    good = ((result_df['Proximity_Score'] >= 60) & (result_df['Proximity_Score'] < 80)).sum()
    fair = ((result_df['Proximity_Score'] >= 40) & (result_df['Proximity_Score'] < 60)).sum()
    poor = (result_df['Proximity_Score'] < 40).sum()
    total = len(result_df)
    
    quality_data = {
        'Excellent (80+)': excellent,
        'Good (60-79)': good,
        'Fair (40-59)': fair,
        'Low (<40)': poor,
    }
    
    fig, ax = plt.subplots(figsize=(11, 4), constrained_layout=True)
    colors = ['#10b981', '#84cc16', '#f59e0b', '#ef4444']
    bars = ax.barh(list(quality_data.keys()), list(quality_data.values()), color=colors)
    ax.set_title(labels.get('graph_quality', 'Keyword Quality Distribution'))
    ax.set_xlabel(labels.get('count', 'Count'))
    ax.grid(axis='x')
    ax.invert_yaxis()
    for i, bar in enumerate(bars):
        width = bar.get_width()
        pct = (width / total * 100) if total > 0 else 0
        ax.text(width + 0.1, bar.get_y() + bar.get_height()/2, f"{int(width)} ({pct:.0f}%)", 
               va='center', fontsize=9)
    charts['quality'] = fig
    
    # 2) Score Distribution
    fig, ax = plt.subplots(figsize=(10.5, 5), constrained_layout=True)
    ax.hist(result_df['Proximity_Score'], bins=20, color='#1d4ed8', edgecolor='white', alpha=0.9)
    ax.set_title(labels.get('graph_distribution', 'Score Distribution'))
    ax.set_xlabel(labels.get('col_score', 'Semantic Proximity Score'))
    ax.set_ylabel(labels.get('count', 'Keyword Count'))
    ax.grid(axis='y')
    charts['distribution'] = fig
    
    # 3) Clicks vs Score (scatter)
    if 'Clicks' in result_df.columns:
        fig, ax = plt.subplots(figsize=(10.5, 5.5), constrained_layout=True)
        scatter = ax.scatter(result_df['Proximity_Score'], result_df['Clicks'],
                           s=80, c=result_df['Proximity_Score'], cmap='viridis', alpha=0.7, edgecolors='white')
        ax.set_title(labels.get('graph_clicks_vs_score', 'Clicks vs Semantic Score'))
        ax.set_xlabel(labels.get('col_score', 'Semantic Proximity Score'))
        ax.set_ylabel(labels.get('col_clicks', 'Clicks'))
        ax.grid()
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label(labels.get('col_score', 'Score'))
        charts['clicks'] = fig

    if 'Position' in result_df.columns:
        fig, ax = plt.subplots(figsize=(10.5, 5.5), constrained_layout=True)
        scatter = ax.scatter(
            result_df['Proximity_Score'],
            result_df['Position'],
            s=75,
            c=result_df['Proximity_Score'],
            cmap='plasma',
            alpha=0.7,
            edgecolors='white'
        )
        ax.set_title(labels.get('graph_position', 'Position vs Semantic Score'))
        ax.set_xlabel(labels.get('col_score', 'Semantic Proximity Score'))
        ax.set_ylabel(labels.get('serp_position', 'SERP Position (lower is better)'))
        ax.invert_yaxis()
        ax.grid()
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label(labels.get('col_score', 'Score'))
        charts['position'] = fig

    if 'Impressions' in result_df.columns:
        opp_df = result_df.copy()
        max_impr = max(opp_df['Impressions'].max(), 1)
        opp_df['Opportunity_Index'] = (100 - opp_df['Proximity_Score']) * np.log1p(opp_df['Impressions']) / np.log1p(max_impr)
        opp_df = opp_df[opp_df['Impressions'] > 0].nlargest(10, 'Opportunity_Index').sort_values('Opportunity_Index')
        if not opp_df.empty:
            fig, ax = plt.subplots(figsize=(10.8, 6.0), constrained_layout=True)
            y_pos = np.arange(len(opp_df))
            ax.hlines(y=y_pos, xmin=0, xmax=opp_df['Opportunity_Index'], color='#93c5fd', linewidth=2)
            ax.scatter(opp_df['Opportunity_Index'], y_pos, color='#1d4ed8', s=85, zorder=3)
            ax.set_yticks(y_pos)
            ax.set_yticklabels(opp_df['Keyword'].astype(str).str.slice(0, 36), fontsize=9)
            ax.set_xlabel(labels.get('opportunity_idx', 'Opportunity Index'))
            ax.set_title(labels.get('graph_opportunity', 'Top SEO Opportunities'))
            ax.grid(axis='x')
            charts['opportunity'] = fig

    fig, ax = plt.subplots(figsize=(8.0, 5.2), constrained_layout=True)
    ax.pie(
        [excellent, good, fair, poor],
        labels=['80+', '60-79', '40-59', '<40'],
        autopct='%1.0f%%',
        colors=['#10b981', '#84cc16', '#f59e0b', '#ef4444'],
        startangle=90
    )
    ax.set_title(labels.get('graph_bucket', 'Semantic Quality Mix'))
    charts['bucket'] = fig
    
    return charts

# ============= MAIN APP =============
def main():
    lang_options = {
        'FranÃ§ais': 'fr',
        'English': 'en',
        'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©': 'ar',
    }
    selected_lang_label = st.selectbox(
        "Language / Langue / Ø§Ù„Ù„ØºØ©",
        list(lang_options.keys()),
        index=0,
    )
    language = lang_options[selected_lang_label]

    if language == 'ar':
        st.markdown(
            """
            <style>
            .stApp { direction: rtl; }
            </style>
            """,
            unsafe_allow_html=True,
        )

    st.title(f"ğŸ“Š {t(language, 'title')}")
    st.markdown(f"*{t(language, 'subtitle')}*")
    
    # Overview
    with st.expander(f"â„¹ï¸ {t(language, 'how_it_works')}", expanded=False):
        st.markdown("""
        ### What This Tool Does
        
        1. **Takes your priority keywords** (the ones you want to rank for)
        2. **Finds the best ranking URL** for each keyword in your GSC data
        3. **Fetches page content** (title, meta, body text)
        4. **Calculates semantic proximity** using AI embeddings (0-100 score)
        5. **Generates insights** with modern visualizations and exportable reports
        
        ### Required Inputs
        
        - **GSC Data (CSV)**: Export from Google Search Console with Query, Page, Clicks, Impressions, Position
        - **Priority Keywords (Excel/CSV)**: Your strategic keywords list with column named "Mot-clÃ©" or "Keyword"
        
        ### Output
        
        - Semantic proximity scores (0-100) for each keyword
        - Quality distribution charts
        - Top/Bottom performing keywords
        - CSV & Excel export with full data
        """)
    
    st.markdown("---")
    
    # Initialize session state
    if 'results' not in st.session_state:
        st.session_state.results = None
    if 'charts' not in st.session_state:
        st.session_state.charts = {}
    
    # Instructions
    st.info(f"ğŸ“‹ **{t(language, 'required_files_info')}**")
    
    # File uploads
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader(f"1ï¸âƒ£ {t(language, 'gsc_label')}")
        st.caption("Export from Google Search Console with Query, Page, Clicks, Impressions, Position columns")
        gsc_file = st.file_uploader(t(language, 'upload_gsc'), type=['csv'], key='gsc', help="Export your GSC data as CSV with queries and landing pages")
    
    with col2:
        st.subheader(f"2ï¸âƒ£ {t(language, 'keywords_label')} â­")
        st.caption("Your strategic keywords list - the app will find best URLs for each and analyze semantic alignment")
        keywords_file = st.file_uploader(t(language, 'upload_keywords'), type=['xlsx', 'xls', 'csv'], key='keywords', help="Excel or CSV file with your priority keywords in 'Mot-clÃ©' or 'Keyword' column")

    matching_mode = st.select_slider(
        f"ğŸ¯ {t(language, 'matching_sensitivity')}",
        options=[t(language, 'strict'), t(language, 'balanced'), t(language, 'wide')],
        value=t(language, 'balanced'),
        help=(
            "Strict = very close query match only, "
            "Balanced = recommended default, "
            "Wide = broader match coverage"
        )
    )
    
    st.markdown("---")
    
    # Analyze button
    if st.button(f"ğŸš€ {t(language, 'analyze_btn')}", type="primary", use_container_width=True):
        if gsc_file and keywords_file:
            with st.spinner("ğŸ”„ Analyzing... (this may take a minute)"):
                try:
                    # Load files
                    gsc_df = pd.read_csv(gsc_file, dtype={'Clicks': str, 'Impressions': str, 'Position': str})

                    prepared_gsc, query_col, page_col = prepare_gsc_data(gsc_df)
                    if prepared_gsc is None:
                        st.error("âŒ Could not find required GSC columns (Query + Page)")
                        st.write(f"Detected columns: {', '.join(gsc_df.columns.astype(str).tolist())}")
                        return
                    
                    # Load keywords from file (handles Excel and CSV)
                    keywords = load_keywords_excel(keywords_file)
                    
                    if not keywords:
                        st.error("âŒ Could not extract keywords from file")
                        st.warning("Make sure your file has a column named 'Mot-clÃ©', 'Keyword', or 'Keywords'")
                        return
                    
                    st.success(f"âœ… Loaded {len(keywords)} priority keywords from file")
                    with st.expander("ğŸ“‹ View loaded keywords"):
                        st.write(", ".join(keywords[:20]))
                        if len(keywords) > 20:
                            st.write(f"... and {len(keywords) - 20} more")
                    
                    st.info(
                        f"ğŸ” Now matching {len(keywords)} keywords with GSC data "
                        f"(mode: {matching_mode}) and analyzing semantic proximity..."
                    )
                    
                    # Load model
                    model = load_embedding_model()
                    if not model:
                        st.error("âŒ Failed to load embedding model")
                        return
                    
                    # Analyze
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    reverse_mode = {
                        t(language, 'strict'): 'Strict',
                        t(language, 'balanced'): 'Balanced',
                        t(language, 'wide'): 'Wide',
                    }
                    selected_mode_en = reverse_mode.get(matching_mode, 'Balanced')
                    mode_attempts = [selected_mode_en]
                    if selected_mode_en != 'Wide':
                        mode_attempts.append('Wide')

                    results = []
                    unmatched_keywords = []
                    matched_keywords_count = 0
                    fetch_failures = 0
                    fetch_failure_details = []
                    effective_mode = selected_mode_en

                    for attempt_index, attempt_mode in enumerate(mode_attempts):
                        if attempt_index > 0:
                            st.warning(
                                "No matches found with the selected mode. "
                                "Retrying automatically with Wide mode..."
                            )

                        results = []
                        unmatched_keywords = []
                        matched_keywords_count = 0
                        fetch_failures = 0
                        fetch_failure_details = []

                        for i, keyword in enumerate(keywords):
                            status_text.text(
                                f"Analyzing ({attempt_mode}): {keyword} ({i+1}/{len(keywords)})"
                            )

                            url_info = get_best_url_for_keyword(
                                prepared_gsc,
                                keyword,
                                query_col,
                                page_col,
                                matching_mode=attempt_mode,
                            )
                            if not url_info:
                                unmatched_keywords.append(keyword)
                                continue

                            matched_keywords_count += 1

                            page_data, fetch_error = fetch_page_content(url_info['url'])
                            if not page_data:
                                fetch_failures += 1
                                fetch_failure_details.append({
                                    'Keyword': keyword,
                                    'Matched_Query': url_info.get('query', ''),
                                    'URL': url_info.get('url', ''),
                                    'Fetch_Error': fetch_error or 'Unknown error',
                                })
                                page_data = {
                                    'title': url_info.get('query', ''),
                                    'meta_description': '',
                                    'content': url_info.get('query', ''),
                                    'url_text': url_info.get('url', ''),
                                    'full_url': url_info.get('url', ''),
                                }

                            proximity = calculate_semantic_proximity(keyword, page_data, model)

                            results.append({
                                'Keyword': keyword,
                                'Matched_Query': url_info['query'],
                                'URL': url_info['url'],
                                'Proximity_Score': proximity,
                                'Clicks': int(url_info['clicks']),
                                'Impressions': int(url_info['impressions']),
                                'Position': round(url_info['position'], 1),
                            })

                            progress_bar.progress((i + 1) / len(keywords))

                        if results:
                            effective_mode = attempt_mode
                            break
                    
                    progress_bar.empty()
                    status_text.empty()
                    
                    if results:
                        result_df = pd.DataFrame(results)
                        st.session_state.results = result_df
                        chart_labels = {
                            'graph_quality': t(language, 'graph_quality'),
                            'graph_distribution': t(language, 'graph_distribution'),
                            'graph_clicks_vs_score': t(language, 'graph_clicks_vs_score'),
                            'graph_position': t(language, 'graph_position'),
                            'graph_opportunity': t(language, 'graph_opportunity'),
                            'graph_bucket': t(language, 'graph_bucket'),
                            'col_score': t(language, 'col_score'),
                            'col_clicks': t(language, 'col_clicks'),
                            'count': t(language, 'count'),
                            'opportunity_idx': t(language, 'opportunity_idx'),
                            'serp_position': t(language, 'serp_position'),
                        }
                        st.session_state.charts = generate_charts(result_df, labels=chart_labels)
                        st.success(
                            f"âœ… Analyzed {len(results)} keywords "
                            f"(matched out of {len(keywords)}) using mode: {effective_mode}"
                        )

                        if fetch_failures > 0:
                            st.warning(
                                f"âš ï¸ {fetch_failures} matched URLs could not be fetched live. "
                                "Fallback semantic inputs were used (query + URL text)."
                            )
                            with st.expander("ğŸ” Fetch failures details"):
                                st.dataframe(
                                    pd.DataFrame(fetch_failure_details),
                                    use_container_width=True,
                                    hide_index=True,
                                )

                        if unmatched_keywords:
                            with st.expander(f"âš ï¸ {len(unmatched_keywords)} keywords had no GSC match"):
                                st.write(", ".join(unmatched_keywords[:50]))
                                if len(unmatched_keywords) > 50:
                                    st.write(f"... and {len(unmatched_keywords) - 50} more")
                    else:
                        if matched_keywords_count == 0:
                            st.error("âŒ No matching keywords found in GSC data")
                        else:
                            st.error(
                                "âŒ Keywords matched in GSC, but no rows were analyzable after processing."
                            )
                        st.info("ğŸ” Quick diagnostic")
                        st.write(f"- Loaded strategic keywords: {len(keywords)}")
                        st.write(f"- GSC rows available: {len(prepared_gsc)}")
                        st.write(f"- GSC query column used: {query_col}")
                        st.write(f"- Matched keywords before fetch: {matched_keywords_count}")
                        st.write(f"- URL fetch failures: {fetch_failures}")
                        sample_queries = prepared_gsc[query_col].dropna().astype(str).head(20).tolist()
                        with st.expander("Sample GSC queries (first 20)"):
                            for query in sample_queries:
                                st.write(f"- {query}")
                        with st.expander("Sample strategic keywords (first 20)"):
                            for keyword in keywords[:20]:
                                st.write(f"- {keyword}")
                
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")
                    import traceback
                    with st.expander("Show detailed error"):
                        st.code(traceback.format_exc())
        else:
            st.error("âš ï¸ **Both files are required to run the analysis:**")
            if not gsc_file:
                st.write("- âŒ GSC Data CSV is missing")
            if not keywords_file:
                st.write("- âŒ **Priority Keywords file is missing** (Excel or CSV with your strategic keywords)")
            
            with st.expander("ğŸ’¡ What are Priority Keywords?"):
                st.markdown("""
                **Priority Keywords** are your strategic target keywords - the ones you want to rank for.
                
                **File Format:**
                - Excel (.xlsx) or CSV file
                - Should contain a column named: `Mot-clÃ©`, `Keyword`, or `Keywords`
                - One keyword per row
                
                **Example:**
                ```
                Mot-clÃ©
                interim
                offre emploi
                agence interim
                recherche emploi
                ```
                
                **The app will:**
                1. Find the best ranking URL in GSC for each keyword
                2. Fetch the page content
                3. Calculate semantic proximity score (0-100)
                4. Generate insights and recommendations
                """)
    
    st.markdown("---")
    
    # Display results
    if st.session_state.results is not None:
        result_df = st.session_state.results
        
        st.subheader(f"ğŸ“ˆ {t(language, 'results_title')}")
        
        # Metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric(t(language, 'avg_score'), f"{result_df['Proximity_Score'].mean():.1f}")
        with col2:
            st.metric(t(language, 'keywords_analyzed'), len(result_df))
        with col3:
            st.metric(t(language, 'avg_clicks'), f"{result_df['Clicks'].mean():.1f}")
        with col4:
            st.metric(t(language, 'avg_impressions'), f"{result_df['Impressions'].mean():.1f}")
        
        st.markdown("---")
        
        # Charts
        st.subheader(f"ğŸ“Š {t(language, 'visualizations')}")
        
        st.markdown(f"**{t(language, 'client_graphs')}**")
        col1, col2 = st.columns(2)
        with col1:
            if 'quality' in st.session_state.charts:
                st.pyplot(st.session_state.charts['quality'], use_container_width=True)
            if 'opportunity' in st.session_state.charts:
                st.pyplot(st.session_state.charts['opportunity'], use_container_width=True)
        with col2:
            if 'bucket' in st.session_state.charts:
                st.pyplot(st.session_state.charts['bucket'], use_container_width=True)
            if 'distribution' in st.session_state.charts:
                st.pyplot(st.session_state.charts['distribution'], use_container_width=True)

        st.markdown(f"**{t(language, 'technical_graphs')}**")
        col3, col4 = st.columns(2)
        with col3:
            if 'clicks' in st.session_state.charts:
                st.pyplot(st.session_state.charts['clicks'], use_container_width=True)
        with col4:
            if 'position' in st.session_state.charts:
                st.pyplot(st.session_state.charts['position'], use_container_width=True)
        
        st.markdown("---")
        
        # Tables
        st.subheader(f"ğŸ¯ {t(language, 'detailed_results')}")
        
        tab1, tab2 = st.tabs([t(language, 'top_keywords'), t(language, 'bottom_keywords')])
        
        with tab1:
            top_20 = result_df.nlargest(20, 'Proximity_Score')
            st.dataframe(top_20, use_container_width=True, hide_index=True)
        
        with tab2:
            bottom_20 = result_df.nsmallest(20, 'Proximity_Score')
            st.dataframe(bottom_20, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # Download
        st.subheader(f"â¬‡ï¸ {t(language, 'export_data')}")
        
        col1, col2 = st.columns(2)
        
        with col1:
            csv = result_df.to_csv(index=False)
            st.download_button(
                label=t(language, 'download_csv'),
                data=csv,
                file_name=f"semantic_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            # Excel export
            buffer = pd.ExcelWriter('temp.xlsx', engine='openpyxl')
            result_df.to_excel(buffer, sheet_name='Results', index=False)
            
            summary = pd.DataFrame({
                'Metric': ['Total Keywords', 'Avg Score', 'Excellent (80+)', 'Good (60-80)', 'Fair (40-60)', 'Poor (<40)'],
                'Value': [
                    len(result_df),
                    f"{result_df['Proximity_Score'].mean():.1f}",
                    (result_df['Proximity_Score'] >= 80).sum(),
                    ((result_df['Proximity_Score'] >= 60) & (result_df['Proximity_Score'] < 80)).sum(),
                    ((result_df['Proximity_Score'] >= 40) & (result_df['Proximity_Score'] < 60)).sum(),
                    (result_df['Proximity_Score'] < 40).sum(),
                ]
            })
            summary.to_excel(buffer, sheet_name='Summary', index=False)
            buffer.close()
            
            with open('temp.xlsx', 'rb') as f:
                st.download_button(
                    label=t(language, 'download_excel'),
                    data=f.read(),
                    file_name=f"semantic_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.ms-excel",
                    use_container_width=True
                )

if __name__ == '__main__':
    main()
